# ü§ñ Agente Aut√≥nomo - Documentaci√≥n Completa

**Versi√≥n:** 2.0.0  
**√öltima actualizaci√≥n:** 2025-12-27  
**Estado:** ‚úÖ Producci√≥n

---

## üìã Tabla de Contenidos

1. [Descripci√≥n General](#-descripci√≥n-general)
2. [Caracter√≠sticas](#-caracter√≠sticas)
3. [Arquitectura](#-arquitectura)
4. [Instalaci√≥n](#-instalaci√≥n)
5. [Configuraci√≥n](#-configuraci√≥n)
6. [Uso](#-uso)
7. [API Reference](#-api-reference)
8. [Desarrollo](#-desarrollo)
9. [Deployment](#-deployment)
10. [Troubleshooting](#-troubleshooting)

---

## üéØ Descripci√≥n General

**Agente Aut√≥nomo** es un sistema de IA conversacional avanzado que combina:
- üß† **M√∫ltiples LLM Providers** (DeepSeek, OpenAI, Anthropic, Ollama)
- üõ†Ô∏è **15+ Herramientas** para gesti√≥n de archivos, comandos y Git
- üí¨ **Interfaz Web** moderna con WebSocket en tiempo real
- üíæ **Persistencia** de conversaciones en SQLite
- üîÑ **Hot-swap** de providers sin reiniciar

### Casos de Uso

- **Desarrollo de Software:** Crear, modificar y gestionar c√≥digo
- **DevOps:** Ejecutar comandos, gestionar infraestructura
- **Automatizaci√≥n:** Scripts, tareas repetitivas, workflows
- **Asistente Personal:** Gesti√≥n de archivos, b√∫squedas, an√°lisis

---

## ‚ú® Caracter√≠sticas

### Backend (FastAPI + Python)

- ‚úÖ **Multi-LLM Support**
  - DeepSeek (recomendado, econ√≥mico)
  - OpenAI (GPT-4, GPT-3.5)
  - Anthropic (Claude 3.5 Sonnet)
  - Ollama (local, gratis)

- ‚úÖ **REST API + WebSocket**
  - Endpoints RESTful para configuraci√≥n
  - WebSocket para streaming en tiempo real
  - CORS configurado para desarrollo

- ‚úÖ **15+ Tools**
  - File operations (read, write, search, delete)
  - Command execution (shell, scripts)
  - Git operations (status, diff, commit, log)

- ‚úÖ **Persistencia**
  - SQLite para conversaciones
  - API keys encriptadas
  - Historial completo de mensajes

### Frontend (HTML + CSS + JavaScript)

- ‚úÖ **Interfaz Moderna**
  - Dise√±o dark mode profesional
  - Glassmorphism effects
  - Responsive design

- ‚úÖ **Features**
  - Chat en tiempo real
  - Historial de conversaciones
  - Selector de provider/modelo
  - Configuraci√≥n de API keys
  - Indicadores de estado

- ‚úÖ **UX**
  - Markdown rendering
  - Code syntax highlighting
  - Tool execution feedback
  - Auto-scroll

---

## üèóÔ∏è Arquitectura

### Diagrama de Componentes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND (Port 3000)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  index.html  ‚îÇ  ‚îÇ   app.js     ‚îÇ  ‚îÇ  style.css   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (UI)        ‚îÇ  ‚îÇ  (Logic)     ‚îÇ  ‚îÇ  (Design)    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    HTTP + WebSocket
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   BACKEND (Port 8000)                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              FastAPI Application                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Routes   ‚îÇ  ‚îÇ WebSocket  ‚îÇ  ‚îÇ   Models   ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚îÇ                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              Agent Core                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ LLM        ‚îÇ  ‚îÇ  Context   ‚îÇ  ‚îÇ   Tools    ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Provider   ‚îÇ  ‚îÇ  Manager   ‚îÇ  ‚îÇ  Registry  ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚îÇ                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              Storage Layer                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ SQLite DB  ‚îÇ  ‚îÇ  API Keys  ‚îÇ  ‚îÇ   Files    ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Mensajes

```
Usuario ‚Üí Frontend ‚Üí WebSocket ‚Üí Backend ‚Üí Agent Core
                                              ‚Üì
                                         LLM Provider
                                              ‚Üì
                                         Tool Execution
                                              ‚Üì
                                         Response Stream
                                              ‚Üì
Frontend ‚Üê WebSocket ‚Üê Backend ‚Üê Agent Core
```

### Estructura de Directorios

```
auto/
‚îú‚îÄ‚îÄ backend/                    # Backend Python
‚îÇ   ‚îú‚îÄ‚îÄ agent/                  # Core del agente
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core.py            # AgentCore principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_provider.py    # Multi-LLM abstraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context.py         # Context manager
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuraci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py         # System prompts
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # App principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py    # DI containers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/            # API endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py        # Chat endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py      # Config endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversations.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocket/         # WebSocket handlers
‚îÇ   ‚îú‚îÄ‚îÄ storage/               # Persistencia
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_storage.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py
‚îÇ   ‚îî‚îÄ‚îÄ tools/                 # Herramientas
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ file_tools.py      # File operations
‚îÇ       ‚îú‚îÄ‚îÄ command_tools.py   # Shell commands
‚îÇ       ‚îî‚îÄ‚îÄ git_tools.py       # Git operations
‚îú‚îÄ‚îÄ frontend/                  # Frontend web
‚îÇ   ‚îú‚îÄ‚îÄ index.html            # UI principal
‚îÇ   ‚îú‚îÄ‚îÄ app.js                # L√≥gica JavaScript
‚îÇ   ‚îú‚îÄ‚îÄ style.css             # Estilos
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ Docs/                     # Documentaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ architecture/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ development/
‚îú‚îÄ‚îÄ scripts/                  # Scripts de utilidad
‚îú‚îÄ‚îÄ tests/                    # Tests
‚îú‚îÄ‚îÄ logs/                     # Logs de aplicaci√≥n
‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python
‚îú‚îÄ‚îÄ start_all.sh             # Iniciar todo
‚îú‚îÄ‚îÄ stop_all.sh              # Detener todo
‚îî‚îÄ‚îÄ README.md                # Este archivo
```

---

## üöÄ Instalaci√≥n

### Requisitos Previos

- **Python:** 3.10 o superior
- **pip:** Gestor de paquetes Python
- **Git:** Para clonar el repositorio
- **Opcional:** Ollama (para LLM local)

### Paso 1: Clonar Repositorio

```bash
git clone https://github.com/tu-usuario/auto.git
cd auto
```

### Paso 2: Crear Entorno Virtual

```bash
# Crear venv
python3 -m venv venv

# Activar (macOS/Linux)
source venv/bin/activate

# Activar (Windows)
venv\\Scripts\\activate
```

### Paso 3: Instalar Dependencias

```bash
pip install -r requirements.txt
```

### Paso 4: Configurar API Keys

Elige uno de los siguientes providers:

#### Opci√≥n A: DeepSeek (Recomendado)

```bash
export DEEPSEEK_API_KEY="sk-..."
```

O gu√°rdalo en `.env`:

```bash
echo "DEEPSEEK_API_KEY=sk-..." > .env
```

#### Opci√≥n B: OpenAI

```bash
export OPENAI_API_KEY="sk-..."
```

#### Opci√≥n C: Anthropic

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

#### Opci√≥n D: Ollama (Local, Gratis)

```bash
# Instalar Ollama
curl https://ollama.ai/install.sh | sh

# Descargar modelo
ollama pull llama3.2:latest

# Iniciar servidor
ollama serve
```

### Paso 5: Iniciar Aplicaci√≥n

```bash
# Opci√≥n 1: Iniciar todo (Backend + Frontend)
./start_all.sh

# Opci√≥n 2: Solo Backend
./start_server.sh

# Opci√≥n 3: Solo Frontend
./start_frontend.sh
```

### Verificar Instalaci√≥n

```bash
# Backend
curl http://localhost:8000/health

# Frontend
open http://localhost:3000
```

---

## ‚öôÔ∏è Configuraci√≥n

### Configuraci√≥n del Backend

El backend se configura mediante:
1. Variables de entorno
2. Archivo `.env`
3. API REST (en tiempo de ejecuci√≥n)

#### Variables de Entorno

```bash
# LLM Provider
DEEPSEEK_API_KEY=sk-...
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# Database
DATABASE_URL=sqlite:///~/.agent_data/conversations/agent.db

# Server
HOST=0.0.0.0
PORT=8000
RELOAD=true
```

#### Cambiar Provider en Tiempo Real

```bash
# Via API
curl -X PUT http://localhost:8000/api/config/ \\
  -H "Content-Type: application/json" \\
  -d '{
    "llm_provider": "deepseek",
    "model": "deepseek-chat"
  }'
```

O desde el frontend:
1. Click en selector "Provider"
2. Seleccionar DeepSeek/OpenAI/Anthropic/Ollama
3. Seleccionar modelo
4. Los cambios se aplican inmediatamente

### Configuraci√≥n del Frontend

El frontend se configura en `frontend/app.js`:

```javascript
const API_URL = 'http://localhost:8000';
const WS_URL = 'ws://localhost:8000';
```

Para producci√≥n, cambiar a tu dominio:

```javascript
const API_URL = 'https://api.tudominio.com';
const WS_URL = 'wss://api.tudominio.com';
```

### Base de Datos

La base de datos SQLite se crea autom√°ticamente en:

```
~/.agent_data/conversations/agent.db
```

**Tablas:**
- `conversations` - Conversaciones
- `messages` - Mensajes
- `api_keys` - API keys (encriptadas)

**Backup:**

```bash
# Backup manual
cp ~/.agent_data/conversations/agent.db backup_$(date +%Y%m%d).db

# Ver datos
sqlite3 ~/.agent_data/conversations/agent.db "SELECT * FROM conversations;"
```

---

## üíª Uso

### Interfaz Web

1. **Abrir navegador:** http://localhost:3000

2. **Configurar API Key (primera vez):**
   - Click en bot√≥n üîë "API Keys"
   - Ingresar API key de DeepSeek/OpenAI/Anthropic
   - Click "üíæ Guardar"

3. **Iniciar conversaci√≥n:**
   - Click "+ Nueva Conversaci√≥n"
   - Escribir mensaje
   - Presionar Enter o click "Enviar"

4. **Cambiar provider:**
   - Usar dropdown "Provider"
   - Seleccionar modelo
   - Los cambios se aplican autom√°ticamente

### Ejemplos de Comandos

#### Gesti√≥n de Archivos

```
"Crea un archivo hello.py con un script que imprima 'Hola Mundo'"
"Lee el contenido de README.md"
"Busca todos los archivos .js en el proyecto"
"Elimina el archivo temp.txt"
```

#### Comandos Shell

```
"Ejecuta 'ls -la' en el directorio actual"
"Instala el paquete requests con pip"
"Lista los procesos de Python corriendo"
"Ejecuta npm install en frontend/"
```

#### Git Operations

```
"Muestra el estado del repositorio Git"
"Haz un commit con mensaje 'feat: add feature'"
"Muestra los √∫ltimos 5 commits"
"Muestra los cambios en app.js"
```

#### Desarrollo

```
"Crea un servidor Express b√°sico en Node.js"
"Escribe tests para la funci√≥n fibonacci"
"Refactoriza el c√≥digo en utils.py"
"Documenta la funci√≥n process_message"
```

### API Program√°tica

```python
import asyncio
from backend.agent import AgentCore, AgentConfig, create_llm_provider
from backend.tools import get_all_tools

async def main():
    # Crear agente
    llm = create_llm_provider("deepseek")
    config = AgentConfig(autonomy_level="semi")
    agent = AgentCore(llm, config)
    
    # Registrar tools
    for tool in get_all_tools():
        agent.register_tool(tool)
    
    # Procesar mensaje
    async for event in agent.process_message(
        "Crea un archivo test.txt",
        "conv_001"
    ):
        if event["type"] == "message":
            print(event["content"])
        elif event["type"] == "tool_call":
            print(f"Ejecutando: {event['tool']}")

asyncio.run(main())
```

---

## üì° API Reference

### REST Endpoints

#### Health Check

```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "service": "agent-api",
  "version": "1.0.0",
  "uptime_seconds": 123.45
}
```

#### Get Configuration

```http
GET /api/config/
```

**Response:**
```json
{
  "llm_provider": "deepseek",
  "model": "deepseek-chat",
  "autonomy_level": "semi",
  "temperature": 0.7,
  "max_tokens": 4000,
  "tools_count": 15
}
```

#### Update Configuration

```http
PUT /api/config/
Content-Type: application/json

{
  "llm_provider": "deepseek",
  "model": "deepseek-chat",
  "api_keys": {
    "deepseek": "sk-..."
  }
}
```

#### List Conversations

```http
GET /api/conversations/
```

**Response:**
```json
[
  {
    "id": "conv_123",
    "title": "Nueva Conversaci√≥n",
    "created_at": "2025-12-27T00:00:00Z",
    "updated_at": "2025-12-27T01:00:00Z",
    "message_count": 5
  }
]
```

#### Get Conversation History

```http
GET /api/chat/{conversation_id}/history
```

**Response:**
```json
{
  "conversation_id": "conv_123",
  "messages": [
    {
      "role": "user",
      "content": "Hola",
      "created_at": "2025-12-27T00:00:00Z"
    },
    {
      "role": "assistant",
      "content": "¬°Hola! ¬øEn qu√© puedo ayudarte?",
      "created_at": "2025-12-27T00:00:01Z"
    }
  ]
}
```

#### Delete Conversation

```http
DELETE /api/conversations/{conversation_id}
```

### WebSocket

#### Connect

```javascript
const ws = new WebSocket('ws://localhost:8000/ws/chat/conv_123');

ws.onopen = () => {
  console.log('Connected');
};

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('Message:', data);
};
```

#### Send Message

```javascript
ws.send(JSON.stringify({
  message: "Hola, ¬øc√≥mo est√°s?"
}));
```

#### Message Types

**Connected:**
```json
{
  "type": "connected"
}
```

**Thinking:**
```json
{
  "type": "thinking"
}
```

**Tool Call:**
```json
{
  "type": "tool_call",
  "tool": "read_file",
  "arguments": {"path": "README.md"}
}
```

**Tool Result:**
```json
{
  "type": "tool_result",
  "tool": "read_file",
  "result": "File content..."
}
```

**Message Chunk (Streaming):**
```json
{
  "type": "message_chunk",
  "content": "Hola"
}
```

**Done:**
```json
{
  "type": "done"
}
```

**Error:**
```json
{
  "type": "error",
  "error": "Error message"
}
```

---

## üõ†Ô∏è Desarrollo

### Setup de Desarrollo

```bash
# Clonar repo
git clone https://github.com/tu-usuario/auto.git
cd auto

# Crear venv
python3 -m venv venv
source venv/bin/activate

# Instalar dependencias de desarrollo
pip install -r requirements.txt
pip install pytest pytest-asyncio pytest-mock black flake8

# Instalar pre-commit hooks
pre-commit install
```

### Ejecutar Tests

```bash
# Todos los tests
pytest

# Tests espec√≠ficos
pytest tests/test_agent.py

# Con coverage
pytest --cov=backend --cov-report=html
```

### Crear Tool Personalizado

```python
# backend/tools/custom_tool.py
from typing import Dict, Any

async def my_custom_tool(param1: str, param2: int) -> Dict[str, Any]:
    \"\"\"
    Descripci√≥n de la herramienta.
    
    Args:
        param1: Descripci√≥n del par√°metro 1
        param2: Descripci√≥n del par√°metro 2
    
    Returns:
        Dict con el resultado
    \"\"\"
    # Tu l√≥gica aqu√≠
    result = f"Procesado: {param1} con {param2}"
    
    return {
        "success": True,
        "result": result
    }

# Registrar en backend/tools/__init__.py
from .custom_tool import my_custom_tool

def get_all_tools():
    return [
        # ... tools existentes
        my_custom_tool,
    ]
```

### Code Style

```bash
# Format code
black backend/ tests/

# Lint
flake8 backend/ tests/

# Type check
mypy backend/
```

---

## üöÄ Deployment

### Producci√≥n con Docker

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY backend/ ./backend/
COPY frontend/ ./frontend/

EXPOSE 8000 3000

CMD ["./start_all.sh"]
```

```bash
# Build
docker build -t agente-autonomo .

# Run
docker run -p 8000:8000 -p 3000:3000 \\
  -e DEEPSEEK_API_KEY=sk-... \\
  agente-autonomo
```

### Producci√≥n con Systemd

```ini
# /etc/systemd/system/agente-backend.service
[Unit]
Description=Agente Aut√≥nomo Backend
After=network.target

[Service]
Type=simple
User=www-data
WorkingDirectory=/opt/agente-autonomo
Environment="DEEPSEEK_API_KEY=sk-..."
ExecStart=/opt/agente-autonomo/venv/bin/python -m uvicorn backend.api.main:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start
sudo systemctl enable agente-backend
sudo systemctl start agente-backend
```

### Nginx Reverse Proxy

```nginx
server {
    listen 80;
    server_name api.tudominio.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## üîß Troubleshooting

### Backend no inicia

**Problema:** `Address already in use`

**Soluci√≥n:**
```bash
# Matar proceso en puerto 8000
lsof -ti:8000 | xargs kill -9

# O usar script
./stop_all.sh
```

### WebSocket se desconecta

**Problema:** WebSocket cierra inmediatamente

**Soluci√≥n:**
- Verificar que backend est√© corriendo
- Revisar logs: `tail -f logs/backend.log`
- Verificar CORS en `backend/api/main.py`

### AI no responde

**Problema:** Mensaje enviado pero sin respuesta

**Soluci√≥n:**
1. Verificar provider configurado:
   ```bash
   curl http://localhost:8000/api/config/
   ```

2. Verificar API key:
   ```bash
   sqlite3 ~/.agent_data/conversations/agent.db "SELECT * FROM api_keys;"
   ```

3. Ver logs de error:
   ```bash
   tail -100 logs/backend.log | grep -i error
   ```

### Error de LLM

**Problema:** `Error en llamada al LLM`

**Soluci√≥n:**
- Verificar API key v√°lida
- Verificar saldo/cr√©ditos
- Si usa Ollama, verificar que est√© corriendo:
  ```bash
  ollama list
  ps aux | grep ollama
  ```

---

## üìö Recursos Adicionales

- **Documentaci√≥n Completa:** [Docs/](Docs/)
- **Ejemplos:** [examples/](examples/)
- **Tests:** [tests/](tests/)
- **Changelog:** [CHANGELOG.md](CHANGELOG.md)
- **Contributing:** [CONTRIBUTING.md](CONTRIBUTING.md)

---

## üìÑ Licencia

MIT License - Ver [LICENSE](LICENSE) para m√°s detalles.

---

## ü§ù Contribuir

¬°Las contribuciones son bienvenidas!

1. Fork el proyecto
2. Crear branch (`git checkout -b feature/amazing-feature`)
3. Commit cambios (`git commit -m 'feat: add amazing feature'`)
4. Push al branch (`git push origin feature/amazing-feature`)
5. Abrir Pull Request

---

## üí¨ Soporte

- **Issues:** [GitHub Issues](https://github.com/tu-usuario/auto/issues)
- **Discussions:** [GitHub Discussions](https://github.com/tu-usuario/auto/discussions)
- **Email:** soporte@tudominio.com

---

**Hecho con ‚ù§Ô∏è por el equipo de Agente Aut√≥nomo**
